import copy
import numpy as np
import torch
import random
import copy
import time

class Algorithms:
    def __init__(self, algorithm=None, compression=None, num_nodes=None, neighbors=None, models=None, data_transform=None,
                 device=None, self_weights=None, neighbor_weights=None, data_loader=None, learning_rate=None,
                 compressors=None, gamma=None, residual_errors=None, self_accumulate_update=None, neighbor_accumulate_update=None,
                 self_H=None, neighbor_H=None, self_G=None, neighbor_G=None, lamda=None, self_update=None,
                 neighbor_update=None, average_rate=None, normalization=None):
        super().__init__()

        "Common"
        self.name = algorithm
        self.compression = compression
        self.num_clients = num_nodes
        self.neighbors = neighbors
        self.device = device
        self.data_transform = data_transform

        self.models = models
        self.self_weights = self_weights
        self.data_loaders = data_loader
        self.learning_rate = learning_rate
        self.compressors = compressors

        "DEFEAT / DEFEAT_Ada"
        self.gamma = gamma
        self.residual_errors = residual_errors
        self.neighbor_weights = neighbor_weights

        "DCD"

        "CHOCO"
        self.self_accumulate_update = self_accumulate_update
        self.neighbor_accumulate_update = neighbor_accumulate_update
        self.consensus = gamma

        "BEER"
        self.self_H = self_H
        self.neighbor_H = neighbor_H
        self.self_G = self_G
        self.neighbor_G = neighbor_G
        self.self_V = []
        self.previous_gradient = []

        "MOTEF"
        self.self_M = []
        self.lamda = lamda

        "DeepSqueeze"
        self.self_update = self_update
        self.neighbor_update = neighbor_update
        self.average_rate = average_rate
        self.tmp_weights = self_update

    def _logger(self):
        print(' compression method:', self.compression, '\n',
              'running algorithm: ', self.name, '\n')

    def _training(self, data_loader, client_weights, model):  # Only consider 1 inner iteration per aggregation
        model.assign_weights(weights=client_weights)
        model.model.train()

        images, labels = data_loader
        images, labels = images.to(self.device), labels.to(self.device)
        if self.data_transform is not None:
            images = self.data_transform(images)

        model.optimizer.zero_grad()
        pred = model.model(images)
        loss = model.loss_function(pred, labels)
        loss.backward()
        model.optimizer.step()

        trained_model = model.get_weights()  # x_t - \eta * gradients
        return trained_model

    def _average_updates(self, updates):
        Averaged_weights = []
        for i in range(self.num_clients):
            Averaged_weights.append(sum(updates[i]) / len(updates[i]))
        return Averaged_weights

    def _averaged_choco(self, updates, update):
        Averaged = []
        for i in range(self.num_clients):
            summation = torch.zeros_like(update[0])
            for j in range(len(updates[i])):
                summation += (1/len(updates[i])) * (updates[i][j] - update[i])
            Averaged.append(summation)
        return Averaged

    def _check_weights(self, client_weights, neighbors_weights):
        checks = 0
        for n in range(self.num_clients):
            neighbors = self.neighbors[n]
            neighbors_models = neighbors_weights[n]

            check = 0
            for m in range(len(neighbors)):
                if torch.equal(neighbors_models[m], client_weights[neighbors[m]]):
                    check += 1
                else:
                    pass
            if check == len(self.neighbors[n]):
                checks += 1
            else:
                pass
        if checks == self.num_clients:
            return True
        else:
            return False

    def DEFEAT(self, iter_num):
        weighted_average = self._average_updates(updates=self.neighbor_weights)
        for n in range(self.num_clients):
            images, labels = next(iter(self.data_loaders[n]))
            trained_weights = self._training(data_loader=[images, labels],
                                           client_weights=self.self_weights[n], model=self.models[n])
            gradients = trained_weights - self.self_weights[n]  # - eta * gradients

            b_t = weighted_average[n] - self.self_weights[n] + gradients + self.gamma * self.residual_errors[n]
            v_t, current_residual = self.compressors[n].get_trans_bits_and_residual(w_tmp=b_t)

            self.residual_errors[n] = current_residual + (1 - self.gamma) * self.residual_errors[n]
            self.self_weights[n] += v_t
            for m in range(self.num_clients):
                if n in self.neighbors[m]:
                    self.neighbor_weights[m][self.neighbors[m].index(n)] += v_t

    def DEFEAT_ada(self, iter_num):
        pass

    def DCD(self, iter_num):
        weighted_average = self._average_updates(updates=self.neighbor_weights)
        for n in range(self.num_clients):
            images, labels = next(iter(self.data_loaders[n]))
            trained_weights = self._training(data_loader=[images, labels],
                                             client_weights=self.self_weights[n], model=self.models[n])
            gradients = trained_weights - self.self_weights[n]  # - eta * gradients

            x_tmp = weighted_average[n] + gradients
            z_t = x_tmp - self.self_weights[n]
            z_t, current_residual = self.compressors[n].get_trans_bits_and_residual(w_tmp=z_t)

            self.self_weights[n] += z_t
            for m in range(self.num_clients):
                if n in self.neighbors[m]:
                    self.neighbor_weights[m][self.neighbors[m].index(n)] += z_t

    def CHOCO(self, iter_num):
        weighted_average = self._averaged_choco(updates=self.neighbor_accumulate_update, update=self.self_accumulate_update)
        for n in range(self.num_clients):
            images, labels = next(iter(self.data_loaders[n]))
            tmp_weight = self._training(data_loader=[images, labels],
                                        client_weights=self.self_weights[n], model=self.models[n])
            self.self_weights[n] = tmp_weight + self.consensus * weighted_average[n]

            q_t = self.self_weights[n] - self.self_accumulate_update[n]
            q_t, current_residual = self.compressors[n].get_trans_bits_and_residual(w_tmp=q_t)

            self.self_accumulate_update[n] += q_t
            for m in range(self.num_clients):
                if n in self.neighbors[m]:
                    self.neighbor_accumulate_update[m][self.neighbors[m].index(n)] += q_t

    def BEER(self, iter_num):
        weighted_average_H = self._averaged_choco(updates=self.neighbor_H, update=self.self_H)
        weighted_average_G = self._averaged_choco(updates=self.neighbor_G, update=self.self_G)

        for n in range(self.num_clients):
            if iter_num == 0:
                images, labels = next(iter(self.data_loaders[n]))
                initial_trained_weight = self._training(data_loader=[images, labels],
                                                        client_weights=self.self_weights[n], model=self.models[n])
                initial_gradient = (self.self_weights[n] - initial_trained_weight) / self.learning_rate
                self.self_V.append(initial_gradient)
                self.previous_gradient.append(initial_gradient)

            self.self_weights[n] = self.self_weights[n] + self.gamma * weighted_average_H[n] - self.learning_rate * self.self_V[n]
            q_h = self.self_weights[n] - self.self_H[n]
            q_h, _ = self.compressors[n].get_trans_bits_and_residual(w_tmp=q_h)

            self.self_H[n] += q_h
            for m in range(self.num_clients):
                if n in self.neighbors[m]:
                    self.neighbor_H[m][self.neighbors[m].index(n)] += q_h

            images, labels = next(iter(self.data_loaders[n]))
            next_trained_weight = self._training(data_loader=[images, labels],
                                                 client_weights=self.self_weights[n], model=self.models[n])
            next_gradient = (self.self_weights[n] - next_trained_weight) / self.learning_rate

            self.self_V[n] = self.self_V[n] + self.gamma * weighted_average_G[n] + next_gradient - self.previous_gradient[n]
            self.previous_gradient[n] = next_gradient

            q_g = self.self_V[n] - self.self_G[n]
            q_g, _ = self.compressors[n].get_trans_bits_and_residual(w_tmp=q_g)

            self.self_G[n] += q_g
            for m in range(self.num_clients):
                if n in self.neighbors[m]:
                    self.neighbor_G[m][self.neighbors[m].index(n)] += q_g

    def MoTEF(self, iter_num):
        if iter_num == 0:
            for n in range(self.num_clients):
                images, labels = next(iter(self.data_loaders[n]))
                initial_trained_weight = self._training(data_loader=[images, labels],
                                                        client_weights=self.self_weights[n], model=self.models[n])
                initial_gradient = (self.self_weights[n] - initial_trained_weight) / self.learning_rate
                self.self_V.append(initial_gradient)
                self.self_M.append(initial_gradient)
                self.self_G[n] = initial_gradient
                self.self_H[n] = self.self_weights[n]
                for m in range(self.num_clients):
                    if n in self.neighbors[m]:
                        self.neighbor_G[m][self.neighbors[m].index(n)] = initial_gradient
                        self.neighbor_H[m][self.neighbors[m].index(n)] = self.self_weights[n]

        weighted_average_H = self._averaged_choco(updates=self.neighbor_H, update=self.self_H)
        weighted_average_G = self._averaged_choco(updates=self.neighbor_G, update=self.self_G)
        for n in range(self.num_clients):
            self.self_weights[n] += self.gamma * weighted_average_H[n] - self.learning_rate * self.self_V[n]
            q_h = self.self_weights[n] - self.self_H[n]
            q_h, _ = self.compressors[n].get_trans_bits_and_residual(w_tmp=q_h)

            self.self_H[n] += q_h
            for m in range(self.num_clients):
                if n in self.neighbors[m]:
                    self.neighbor_H[m][self.neighbors[m].index(n)] += q_h

            images, labels = next(iter(self.data_loaders[n]))
            next_trained_weight = self._training(data_loader=[images, labels],
                                                 client_weights=self.self_weights[n], model=self.models[n])
            next_gradient = (self.self_weights[n] - next_trained_weight) / self.learning_rate
            current_M = copy.deepcopy(self.self_M[n])
            self.self_M[n] = (1 - self.lamda) * self.self_M[n] + self.lamda * next_gradient
            self.self_V[n] += self.gamma * weighted_average_G[n] + self.self_M[n] - current_M

            q_g = self.self_V[n] - self.self_G[n]
            q_g, _ = self.compressors[n].get_trans_bits_and_residual(w_tmp=q_g)

            self.self_G[n] += q_g
            for m in range(self.num_clients):
                if n in self.neighbors[m]:
                    self.neighbor_G[m][self.neighbors[m].index(n)] += q_g

    def DeepsSqueeze(self, iter_num):  # Need to be checked, the original DeepSqueeze algorithm is not correct.
        for n in range(self.num_clients):
            images, labels = next(iter(self.data_loaders[n]))
            current_trained_weight = self._training(data_loader=[images, labels],
                                                    client_weights=self.self_weights[n], model=self.models[n])
            current_gradient = (self.self_weights[n] - current_trained_weight) / self.learning_rate

            self.tmp_weights[n] = self.self_weights[n] - self.learning_rate * current_gradient
            v_t = self.self_weights[n] - self.learning_rate * current_gradient + self.residual_errors[n]
            compressed_v, self.residual_errors[n] = self.compressors[n].get_trans_bits_and_residual(w_tmp=v_t)

            self.self_update[n] = compressed_v
            for m in range(self.num_clients):
                if n in self.neighbors[m]:
                    self.neighbor_update[m][self.neighbors[m].index(n)] = compressed_v

        weighted_average_update = self._averaged_choco(updates=self.neighbor_update, update=self.self_update)

        for n in range(self.num_clients):
            self.self_weights[n] = self.tmp_weights[n] + self.average_rate * weighted_average_update[n]

    def CEDAS(self):
        pass

    def DeCoM(self):
        pass
